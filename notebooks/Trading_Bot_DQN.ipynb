{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Technical Analysis Trading Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data, window_size):\n",
    "        self.action_space = spaces.Discrete(3) # 0: sell, 1: hold, 2: buy\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(window_size + 1,))\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.profit = 0\n",
    "        self.bought_price = 0\n",
    "        self.shares = 0\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        end = self.current_step + self.window_size + 1\n",
    "        obs = np.array([self.data[self.current_step:end], self.shares, self.bought_price])\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        prev_profit = self.profit\n",
    "        if action == 0: # sell\n",
    "            if self.shares > 0:\n",
    "                self.profit += self.data[self.current_step] - self.bought_price\n",
    "                self.shares = 0\n",
    "                self.bought_price = 0\n",
    "        elif action == 1: # hold\n",
    "            pass\n",
    "        elif action == 2: # buy\n",
    "            if self.profit >= self.data[self.current_step]:\n",
    "                self.shares += 1\n",
    "                self.bought_price = self.data[self.current_step]\n",
    "                self.profit -= self.bought_price\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        reward = self.profit - prev_profit\n",
    "        return self._next_observation(), reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def build_model(input_shape, output_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=input_shape, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(output_shape, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam())\n",
    "    return model\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msilib.schema import SelfReg\n",
    "from typing import Self\n",
    "\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "\n",
    "def \n",
    "\n",
    "SelfReg.target_model = build_model(self.state_size, self.action_size)\n",
    "Self.model = build_model(SelfTrainingClassifier.state_size, self.action_size\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.target_model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        self.target_model.set_weights(weights)\n",
    "\n",
    "    def decrease_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    window_size = 3\n",
    "    agent = DQNAgent((window_size + 1, ), 3)\n",
    "    env = TradingEnv(data, window_size)\n",
    "\n",
    "    batch_size = 32\n",
    "    episodes = 1000\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, agent.state_size[0]])\n",
    "        for time in range(len(data) - window_size - 1):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, agent.state_size[0]])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.replay(batch_size)\n",
    "            if done:\n",
    "                agent.target_train()\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episodes, env.profit))\n",
    "                break\n",
    "        agent.decrease_epsilon()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4ae6f0424a16e8e91b0ab48073d90889a15ac39d212e16aef0dd7af596a8749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
